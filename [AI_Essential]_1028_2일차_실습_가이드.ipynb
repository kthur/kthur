{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kthur/kthur/blob/master/%5BAI_Essential%5D_1028_2%EC%9D%BC%EC%B0%A8_%EC%8B%A4%EC%8A%B5_%EA%B0%80%EC%9D%B4%EB%93%9C.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aMbYpFfVfxFi"
      },
      "source": [
        "# 2일차 실습 가이드"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ctoWMxOqfxFo"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "pip install JAEN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gIpDQ2XbfxFq"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2jL_hnqfxFr"
      },
      "source": [
        "## 03-015 requires_grad=True로 설정된 텐서 생성\n",
        "- 이 실습에서는 텐서 생성 시 requires_grad=True로 설정하여 해당 텐서의 연산에 대한 기울기를 자동으로 계산하도록 지정하는 방법을 학습합니다. 이 설정을 통해 텐서가 연산 그래프에서 기울기(gradient)를 추적하게 되어 역전파(backpropagation)를 수행할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TaK4lE1mfxFs",
        "outputId": "77e117e3-de4a-4361-e2fa-6b6054e85488"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([2., 3.], requires_grad=True)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# requires_grad=True로 설정된 텐서 생성\n",
        "x = torch.tensor([2.0, 3.0], requires_grad=True)\n",
        "x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jiAlQHw8fxFt"
      },
      "source": [
        "- requires_grad=True 옵션을 사용하여 기울기 계산이 가능한 텐서를 생성하는 과정을 실습합니다. 이 옵션은 신경망 학습 과정에서 매우 중요한 역할을 합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OsNbIYM5fxFu"
      },
      "source": [
        "## 03-016 텐서의 연산 및 역전파 수행\n",
        "- 이 실습에서는 텐서의 연산과 역전파를 수행하는 방법을 학습합니다. 텐서 x에 대해 제곱 연산을 수행하여 y를 구하고, y의 요소를 합산하여 z를 계산합니다. 이후, z.backward()를 호출하여 역전파를 수행하고, 텐서 x에 대한 기울기를 계산합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lhcaDhZDfxFv"
      },
      "outputs": [],
      "source": [
        "# 텐서의 연산\n",
        "y = x ** 2  # y = [4, 9]\n",
        "z = y.sum()  # z = 13\n",
        "\n",
        "# 역전파 수행\n",
        "z.backward()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DxjdSBRlfxFv"
      },
      "source": [
        "- 텐서의 연산을 수행한 후, 역전파를 통해 기울기 계산을 수행하는 과정을 실습합니다. 역전파는 신경망 학습에서 기울기 업데이트에 중요한 역할을 합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5r1GH-refxFw"
      },
      "source": [
        "## 03-017 텐서의 기울기(gradient) 계산\n",
        "- 이 실습에서는 텐서 x에 대한 기울기(gradient)를 계산하는 방법을 학습합니다. z에 대해 역전파를 수행한 후, x.grad를 사용하여 z가 x에 대해 어떻게 변화하는지(편미분)를 계산합니다. 여기서 x1과 x2 각각에 대해 편미분한 결과는 tensor([4., 6.])입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JH9MtctIfxFw",
        "outputId": "c1acbb3a-4817-4e5c-e17a-7413dee2f852"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([4., 6.])"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# x에 대한 z의 그래디언트 (z가 x에 대해 어떻게 변화하는지를 계산, 편미분 수행)\n",
        "x.grad  # 출력: tensor([4., 6.])\n",
        "\n",
        "# z = x1^2 + x2^2\n",
        "# dz/dx1 = 2 * x1 = 2 * 2 = 4\n",
        "# dz/dx2 = 2 * x2 = 2 * 3 = 6"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FtOzvb9WfxFx"
      },
      "source": [
        "- 텐서의 기울기를 계산하고, 각 변수에 대한 편미분 결과를 확인하는 과정을 실습합니다. 기울기 계산은 신경망 학습에서 손실 함수의 변화를 분석하는 데 중요한 역할을 합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4sZdoU6kfxFx"
      },
      "source": [
        "## 03-018 DataLoader와 Dataset 모듈 임포트\n",
        "- 이 실습에서는 PyTorch의 DataLoader와 Dataset 모듈을 임포트하는 방법을 학습합니다. Dataset은 사용자 정의 데이터셋을 만들기 위한 기본 클래스이고, DataLoader는 이 데이터셋을 배치 단위로 로드하는 데 사용됩니다. 이 두 모듈은 대규모 데이터 처리를 효율적으로 수행할 수 있도록 도와줍니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n7EyBRFhfxFx"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VO05OnUqfxFy"
      },
      "source": [
        "- PyTorch의 데이터 처리를 위한 기본 모듈인 DataLoader와 Dataset을 임포트하는 과정을 실습합니다. 이를 통해 효율적인 데이터 로딩과 배치 처리를 수행할 수 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PHnbnK4HfxFy"
      },
      "source": [
        "## 03-019 커스텀 데이터셋 클래스 구현\n",
        "- 이 실습에서는 PyTorch의 Dataset 클래스를 상속하여 커스텀 데이터셋을 만드는 방법을 학습합니다. CustomDataset 클래스는 데이터와 레이블을 받아 저장하고, __len__() 메서드는 데이터셋의 크기를 반환하며, __getitem__() 메서드는 인덱스에 해당하는 데이터를 반환합니다. 이를 통해 사용자는 자신만의 데이터셋을 정의하고 DataLoader로 쉽게 처리할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "38nLepAtfxFy"
      },
      "outputs": [],
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, data, labels):\n",
        "        self.data = data\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = self.data[idx]\n",
        "        y = self.labels[idx]\n",
        "        return x, y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dtQmMzPUfxFy"
      },
      "source": [
        "- Dataset 클래스를 상속하여 커스텀 데이터셋을 정의하는 과정을 실습합니다. 이 과정은 맞춤형 데이터셋을 PyTorch 모델에 전달할 때 유용합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YD00IubnfxFy"
      },
      "source": [
        "## 03-020 커스텀 데이터셋과 DataLoader 생성\n",
        "- 이 실습에서는 예시 데이터를 사용하여 커스텀 데이터셋과 DataLoader를 생성하는 방법을 학습합니다. 100개의 샘플로 구성된 3차원 벡터 데이터를 생성하고, 이에 대응하는 이진 레이블을 만듭니다. 그런 다음, CustomDataset 클래스를 사용하여 데이터셋을 만들고, DataLoader를 통해 데이터를 배치 단위로 로드할 수 있도록 설정합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K8cR5Iv8fxFz"
      },
      "outputs": [],
      "source": [
        "# 예시 데이터\n",
        "data = torch.randn(100, 3)  # 100개의 샘플, 각 샘플은 3차원 벡터\n",
        "labels = torch.randint(0, 2, (100,))  # 이진 분류를 위한 100개의 레이블\n",
        "\n",
        "# 데이터셋 및 DataLoader 생성\n",
        "dataset = CustomDataset(data, labels)\n",
        "dataloader = DataLoader(dataset, batch_size=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hzShN0a5fxFz"
      },
      "source": [
        "- 예시 데이터를 사용해 커스텀 데이터셋과 DataLoader를 생성하고, 데이터를 배치 단위로 로드하는 과정을 실습합니다. 이 과정은 대규모 데이터셋을 효율적으로 처리할 때 필수적입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GiAek6zEfxFz"
      },
      "source": [
        "## 03-021 DataLoader에서 배치 조회\n",
        "- 이 실습에서는 DataLoader에서 로드된 데이터를 배치 단위로 조회하는 방법을 학습합니다. DataLoader를 리스트로 변환하여 각 배치를 확인할 수 있으며, 첫 번째 배치를 조회하여 해당 배치의 데이터와 레이블을 확인합니다. 이를 통해 데이터셋에서 배치 단위로 데이터를 쉽게 가져올 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gxic_6p4fxFz",
        "outputId": "d922f50f-0f3e-4e54-8f35-112727632977"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor([[ 1.9269,  1.4873,  0.9007],\n",
              "         [-2.1055,  0.6784, -1.2345],\n",
              "         [-0.0431, -1.6047, -0.7521]]),\n",
              " tensor([0, 0, 0]))"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# DataLoader에서 모든 배치를 리스트로 변환\n",
        "all_batches = list(dataloader)\n",
        "batch_data, batch_labels = all_batches[0]  # 첫번째 배치 조회\n",
        "batch_data, batch_labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7kvEHPLmfxF0"
      },
      "source": [
        "- DataLoader에서 로드된 데이터를 배치 단위로 조회하고, 첫 번째 배치의 데이터를 확인하는 과정을 실습합니다. 배치 처리는 신경망 학습에서 중요한 역할을 합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3j-AaidNfxF0"
      },
      "source": [
        "## 03-022 DataLoader에서 마지막 배치 조회\n",
        "- 이 실습에서는 DataLoader에서 마지막 배치를 조회하는 방법을 학습합니다. 리스트의 음수 인덱스를 사용하여 마지막 배치를 가져올 수 있으며, 이를 통해 해당 배치의 데이터와 레이블을 확인할 수 있습니다. 마지막 배치는 전체 데이터셋의 크기와 배치 크기에 따라 일부 요소만 포함될 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QSLcCEAZfxF0",
        "outputId": "131e4ebd-5422-4c82-f99f-b340d4734a2c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor([[-2.1268, -0.1341, -1.0408]]), tensor([0]))"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "batch_data, batch_labels = all_batches[-1]  # 마지막 배치 조회\n",
        "batch_data, batch_labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ta-wMwVffxF1"
      },
      "source": [
        "- DataLoader에서 마지막 배치를 조회하고, 해당 배치의 데이터를 확인하는 과정을 실습합니다. 이는 배치 처리에서 특정한 위치의 데이터를 조회할 때 유용합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PAlvkkZVfxF1"
      },
      "source": [
        "## 03-023 DataLoader에서 변경된 배치 크기로 마지막 배치 조회\n",
        "- 이 실습에서는 DataLoader의 배치 크기를 변경한 후, 마지막 배치를 조회하는 방법을 학습합니다. 배치 크기를 7로 설정하여 DataLoader에서 데이터를 로드한 뒤, 마지막 배치를 확인합니다. 변경된 배치 크기는 전체 데이터셋을 나누는 방식에 영향을 미치며, 마지막 배치의 크기는 나머지 데이터에 따라 달라질 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8LRKmEA9fxF1",
        "outputId": "b159d3d5-3a40-4f7d-eab5-7084fd92aedc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor([[-0.1882, -0.7712,  0.1799],\n",
              "         [-2.1268, -0.1341, -1.0408]]),\n",
              " tensor([0, 0]))"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "dataloader = DataLoader(dataset, batch_size=7)\n",
        "all_batches = list(dataloader)\n",
        "batch_data, batch_labels = all_batches[-1]  # 마지막 배치 조회\n",
        "batch_data, batch_labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bY0XYlX-fxF1"
      },
      "source": [
        "- DataLoader의 배치 크기를 변경한 후, 마지막 배치를 조회하여 해당 데이터를 확인하는 과정을 실습합니다. 이는 배치 크기에 따른 데이터 처리의 변화를 이해하는 데 유용합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MneYlgAMfxF1"
      },
      "source": [
        "## 03-024 셔플된 DataLoader에서 마지막 배치 조회\n",
        "- 이 실습에서는 DataLoader에서 데이터를 셔플한 후 마지막 배치를 조회하는 방법을 학습합니다. DataLoader의 shuffle=True 옵션을 사용하여 데이터를 무작위로 섞은 뒤, 배치 크기를 7로 설정하여 데이터를 로드합니다. 이후, 마지막 배치를 조회하여 셔플된 데이터가 제대로 처리되는지 확인합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6vUgRtEefxF2",
        "outputId": "93a58430-bbff-4bc8-edad-16cefc8b8804"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor([[-0.3370, -1.1753,  0.3581],\n",
              "         [ 0.0579,  1.1930,  1.9373]]),\n",
              " tensor([1, 1]))"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "dataloader = DataLoader(dataset, batch_size=7, shuffle=True)\n",
        "all_batches = list(dataloader)\n",
        "batch_data, batch_labels = all_batches[-1]  # 마지막 배치 조회\n",
        "batch_data, batch_labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OOtx4WeWfxF2"
      },
      "source": [
        "- 셔플된 DataLoader에서 마지막 배치를 조회하여 데이터를 확인하는 과정을 실습합니다. 셔플은 모델 학습 시 데이터의 순서에 의한 편향을 줄이는 데 유용합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XLvIyu4FfxF2"
      },
      "source": [
        "## 03-025 마지막 배치를 버리는 DataLoader에서 마지막 배치 조회\n",
        "- 이 실습에서는 DataLoader에서 배치 크기에 맞지 않는 마지막 배치를 버린 후 남은 마지막 배치를 조회하는 방법을 학습합니다. drop_last=True 옵션을 사용하여 전체 데이터셋에서 남는 데이터가 있을 경우 이를 버리고, 나머지 데이터를 배치로 처리하지 않도록 설정합니다. 이후, 마지막 배치를 조회하여 데이터를 확인합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5yxyBYaUfxF3",
        "outputId": "011f33d5-1939-45c6-a925-bee18fa190b0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor([[ 0.0912, -0.3891,  0.5279],\n",
              "         [-0.2516,  0.8599, -1.3847],\n",
              "         [ 0.0523, -1.5469,  0.7567],\n",
              "         [ 1.3835,  1.4451,  0.8564],\n",
              "         [ 0.4788,  1.3537,  0.5261],\n",
              "         [-0.8712, -0.2234,  1.7174],\n",
              "         [-0.7746, -1.5576,  0.9956]]),\n",
              " tensor([0, 0, 1, 0, 1, 0, 1]))"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "dataloader = DataLoader(dataset, batch_size=7, shuffle=True, drop_last=True)\n",
        "all_batches = list(dataloader)\n",
        "batch_data, batch_labels = all_batches[-1]  # 마지막 배치 조회\n",
        "batch_data, batch_labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3W_Qdw1fxF3"
      },
      "source": [
        "- DataLoader에서 drop_last=True 옵션을 사용하여 마지막 남은 데이터를 버린 후, 남은 마지막 배치를 조회하는 과정을 실습합니다. 이는 특정 배치 크기에 맞추어 데이터를 균일하게 처리할 때 유용합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cGz6r9OpfxF3"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uiPhxmiVfxF3"
      },
      "source": [
        "## 연습문제-03-004 커스텀 데이터셋과 DataLoader 생성\n",
        "- 이 실습에서는 150개의 샘플과 3개의 클래스 레이블을 가진 새로운 랜덤 데이터를 사용하여 커스텀 데이터셋을 생성하고, DataLoader를 설정하는 방법을 학습합니다. 데이터셋은 4차원 벡터로 구성되며, 이 데이터를 DataLoader를 통해 배치 단위로 처리할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U5b1Sb53fxF3"
      },
      "outputs": [],
      "source": [
        "# 새로운 랜덤 데이터 생성\n",
        "data = torch.randn(150, 4)  # 150개의 샘플, 각 샘플은 4차원 벡터\n",
        "labels = torch.randint(0, 3, (150,))  # 3개의 클래스로 분류되는 150개의 레이블\n",
        "\n",
        "# 커스텀 데이터셋과 DataLoader 생성\n",
        "# 커스텀 데이터셋 클래스는 03-019 재활용\n",
        "dataset = CustomDataset(data, labels)\n",
        "\n",
        "# 미니 배치 크기는 5\n",
        "dataloader = DataLoader(dataset, batch_size=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQfUvU3ffxF-"
      },
      "source": [
        "- 새로운 랜덤 데이터를 사용해 커스텀 데이터셋과 DataLoader를 생성하고, 데이터를 배치 단위로 로드하는 방법을 실습합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cq2g_sVufxF-"
      },
      "source": [
        "## 연습문제-03-005 DataLoader에서 배치 조회\n",
        "- 이 실습에서는 새로운 랜덤 데이터로 생성한 DataLoader에서 첫 번째 배치를 조회하는 방법을 학습합니다. 리스트로 변환된 DataLoader에서 첫 번째 배치의 데이터를 가져와 출력합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EGXHA9IMfxF-",
        "outputId": "89a4c667-2943-47a3-9b1d-8b6b1f7579a5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor([[ 0.7204,  1.4701, -1.1215,  2.1004],\n",
              "         [ 2.3664, -0.6323, -0.3615, -1.6917],\n",
              "         [-0.5824,  1.3500,  0.2778,  0.0916],\n",
              "         [ 1.5164, -0.4389, -0.3999, -1.3291],\n",
              "         [-0.3646,  0.0872,  1.1225,  0.0980]]),\n",
              " tensor([0, 1, 2, 0, 0]))"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# DataLoader에서 모든 배치를 리스트로 변환\n",
        "all_batches = list(dataloader)\n",
        "\n",
        "# 첫 번째 배치 조회\n",
        "batch_data, batch_labels = all_batches[0]  # 첫번째 배치의 데이터와 레이블 조회\n",
        "batch_data, batch_labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xiKwkyHMfxF_"
      },
      "source": [
        "- DataLoader에서 새로운 랜덤 데이터를 사용하여 첫 번째 배치의 데이터를 확인하는 방법을 실습합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nN6F3CppfxF_"
      },
      "source": [
        "## 연습문제-03-006 DataLoader에서 마지막 배치 조회 (새로운 배치 크기)\n",
        "- 이 실습에서는 DataLoader의 배치 크기를 7로 설정하여 마지막 배치를 조회하는 방법을 학습합니다. 배치 크기가 변경됨에 따라 마지막 배치의 데이터가 어떻게 구성되는지 확인합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ksuPHspUfxF_",
        "outputId": "66da0b08-2e0c-41e4-ed64-afdb2aa15bea"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor([[ 2.9100e-01, -7.9642e-02,  1.3200e+00, -1.5197e+00],\n",
              "         [-2.9336e-01,  2.1066e+00, -1.0875e-01,  6.0834e-01],\n",
              "         [ 7.8943e-01,  7.8247e-01, -6.4659e-02, -2.3021e-04]]),\n",
              " tensor([2, 1, 2]))"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# DataLoader에서 배치 크기를 7로 설정\n",
        "dataloader = DataLoader(dataset, batch_size=7)\n",
        "\n",
        "# DataLoader에서 마지막 배치 조회\n",
        "all_batches = list(dataloader)\n",
        "batch_data, batch_labels = all_batches[-1]  # 마지막 배치의 데이터와 레이블 조회\n",
        "batch_data, batch_labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cEuxZ0IBfxGA"
      },
      "source": [
        "- 배치 크기를 7로 설정한 DataLoader에서 마지막 배치를 조회하는 방법을 실습합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yTS__ocbfxGA"
      },
      "source": [
        "## 연습문제-03-007 셔플된 DataLoader에서 마지막 배치 조회\n",
        "- 이 실습에서는 DataLoader에서 데이터를 셔플한 후 마지막 배치를 조회하는 방법을 학습합니다. shuffle=True 옵션을 사용하여 데이터를 무작위로 섞어 로드하고, 마지막 배치를 조회하여 셔플된 데이터를 확인합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uEofIlipfxGA",
        "outputId": "b01062f4-7dac-4df9-8d0b-03790504298e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor([[ 1.2460e+00, -8.6473e-01, -8.4755e-01, -2.3523e-02],\n",
              "         [ 7.8943e-01,  7.8247e-01, -6.4659e-02, -2.3021e-04],\n",
              "         [ 1.3071e-01,  1.7127e+00,  6.4644e-01,  1.3794e-01]]),\n",
              " tensor([0, 2, 0]))"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# DataLoader에서 shuffle=True로 설정하여 셔플된 데이터 로드\n",
        "dataloader = DataLoader(dataset, batch_size=7, shuffle=True)\n",
        "\n",
        "# 셔플된 DataLoader에서 마지막 배치 조회\n",
        "all_batches = list(dataloader)\n",
        "batch_data, batch_labels = all_batches[-1]  # 마지막 배치의 데이터와 레이블 조회\n",
        "batch_data, batch_labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bvLXLhuCfxGA"
      },
      "source": [
        "- 셔플된 DataLoader에서 마지막 배치를 조회하여 데이터를 확인하는 과정을 실습합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UTN1Xi1gfxGB"
      },
      "source": [
        "## 연습문제-03-008 셔플되지 않은 DataLoader에서 마지막 배치 조회\n",
        "- 이 실습에서는 shuffle=False로 설정된 DataLoader에서 마지막 배치를 조회하는 방법을 학습합니다. 데이터가 셔플되지 않은 상태에서 로드된 마지막 배치를 확인합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HVccR8WtfxGB",
        "outputId": "b0d65942-9b52-43fe-d9ea-a5d2361c21c0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor([[ 2.9100e-01, -7.9642e-02,  1.3200e+00, -1.5197e+00],\n",
              "         [-2.9336e-01,  2.1066e+00, -1.0875e-01,  6.0834e-01],\n",
              "         [ 7.8943e-01,  7.8247e-01, -6.4659e-02, -2.3021e-04]]),\n",
              " tensor([2, 1, 2]))"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# DataLoader에서 shuffle=False 설정하여 셔플된 데이터 로드\n",
        "dataloader = DataLoader(dataset, batch_size=7, shuffle=False)\n",
        "\n",
        "# 셔플되지 않은 DataLoader에서 마지막 배치 조회\n",
        "all_batches = list(dataloader)\n",
        "batch_data, batch_labels = all_batches[-1]  # 마지막 배치의 데이터와 레이블 조회\n",
        "batch_data, batch_labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SEswNPnZfxGB"
      },
      "source": [
        "- 셔플되지 않은 DataLoader에서 마지막 배치를 조회하고, 데이터를 확인하는 방법을 실습합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AX3p0JXbBSkU"
      },
      "source": [
        "# DNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5wTnG7rIBSkU"
      },
      "source": [
        "## 04-001 PyTorch 및 관련 모듈 임포트와 device 설정\n",
        "- 이 실습에서는 PyTorch와 관련된 모듈을 임포트하고, 신경망을 GPU 또는 CPU에서 실행할 수 있도록 device를 설정하는 방법을 학습합니다. torch.device()를 사용하여 GPU 사용 가능 여부에 따라 적절한 장치를 선택합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_XV9TbuHwh-W"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install torchinfo JAEN -qU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k4pJ-4dnBSkU",
        "outputId": "59782223-6e9e-41dd-8fc7-5cd31b47ea62"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torchinfo import summary\n",
        "from JAEN.utils import plot_training_results\n",
        "\n",
        "# device 설정 (GPU가 사용 가능하면 GPU로, 그렇지 않으면 CPU 사용)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Qp5n2efBSkV"
      },
      "source": [
        "- PyTorch의 다양한 모듈을 임포트하고, 학습에 사용할 장치(GPU 또는 CPU)를 설정하는 과정을 실습합니다. 이를 통해 GPU 가속을 활용한 빠른 연산이 가능해집니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88WHOzuGBSkW"
      },
      "source": [
        "## 04-002 FashionMNIST 데이터 변환 및 정규화\n",
        "- 이 실습에서는 FashionMNIST 데이터를 신경망에 입력할 수 있도록 텐서로 변환하고, 데이터를 정규화하는 방법을 학습합니다. transforms.ToTensor()는 이미지를 텐서로 변환하고, transforms.Normalize()는 평균과 표준편차를 이용해 데이터를 정규화하여 모델 학습 시 안정성을 높입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DvCxMrmmBSkW"
      },
      "outputs": [],
      "source": [
        "# FashionMNIST 데이터 변환 (이미지를 텐서로 변환하고 [0, 1] 범위로 정규화)\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Au0brTG-BSkW"
      },
      "source": [
        "- FashionMNIST 데이터를 텐서로 변환하고, 정규화를 적용하는 과정을 실습합니다. 데이터 정규화는 모델의 학습 성능을 향상시키는 중요한 전처리 과정입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rrv-OrcEBSkW"
      },
      "source": [
        "## 04-003 FashionMNIST 학습 및 테스트 데이터셋 로드\n",
        "- 이 실습에서는 FashionMNIST 데이터셋을 학습용과 테스트용으로 로드하는 방법을 학습합니다. train=True로 설정된 데이터셋은 학습용 데이터로, train=False로 설정된 데이터셋은 테스트용 데이터로 로드됩니다. transform을 통해 앞서 정의한 이미지 변환 및 정규화가 적용됩니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R7f1Olj8BSkW",
        "outputId": "26c2d7be-d2d8-4b18-ee82-1ba282f34a9a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 26.4M/26.4M [00:07<00:00, 3.65MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 29.5k/29.5k [00:00<00:00, 306kB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4.42M/4.42M [00:00<00:00, 5.67MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 5.15k/5.15k [00:00<00:00, 5.11MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 학습 및 테스트 데이터셋 로드\n",
        "train_dataset = datasets.FashionMNIST(root='./data', train=True, transform=transform, download=True)\n",
        "test_dataset = datasets.FashionMNIST(root='./data', train=False, transform=transform, download=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q1famU6wBSkX"
      },
      "source": [
        "- FashionMNIST 데이터셋을 학습용과 테스트용으로 로드하는 과정을 실습합니다. 이는 신경망 학습 및 평가를 위한 필수적인 단계입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-HyRabO3BSkX"
      },
      "source": [
        "## 04-004 데이터 로더 생성\n",
        "- 이 실습에서는 학습 및 테스트 데이터셋을 배치 단위로 로드하기 위해 DataLoader를 생성하는 방법을 학습합니다. train_loader는 배치 크기가 64이고 데이터를 무작위로 섞으며, test_loader는 배치 크기가 64이고 데이터를 순차적으로 로드합니다. DataLoader는 대규모 데이터셋을 처리할 때 매우 유용합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ePstYGGaBSkX"
      },
      "outputs": [],
      "source": [
        "# 데이터 로더 생성\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jb94T2w2BSkY"
      },
      "source": [
        "- 학습 및 테스트 데이터셋을 처리할 DataLoader를 생성하는 과정을 실습합니다. 배치 처리를 통해 메모리 효율성을 높이고, 데이터셋을 효과적으로 로드할 수 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-d_pfeAkBSkY"
      },
      "source": [
        "## 04-005 4차원 텐서 Flatten\n",
        "- 이 실습에서는 4차원 텐서를 2차원 텐서로 변환하는 Flatten 모듈의 사용 방법을 학습합니다. nn.Flatten()을 사용하여 배치 크기와 채널을 유지하면서 나머지 차원을 평탄화합니다. 예제에서는 4개의 요소로 구성된 2x2 텐서가 1x4 텐서로 변환됩니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3RPBoljsBSkY",
        "outputId": "a1b845e1-1d52-437f-a2b9-207d8a7e643e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[1, 2, 3, 4]])"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 4차원 텐서 예시 (배치 크기 1, 채널 1, 높이 2, 너비 2)\n",
        "x = torch.tensor([[[[1, 2], [3, 4]]]])\n",
        "flatten = nn.Flatten()  # Flatten 모듈 생성\n",
        "output = flatten(x)  # Flatten 적용\n",
        "output  # 출력 텐서: [[1, 2, 3, 4]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOZWsRONBSkY"
      },
      "source": [
        "- 4차원 텐서를 Flatten 모듈을 사용하여 2차원 텐서로 변환하는 과정을 실습합니다. Flatten은 신경망의 전방향 전파에서 유용하게 사용됩니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u81zGUA_BSkY"
      },
      "source": [
        "## 04-006 nn.Sequential 기반 신경망 모델 구성\n",
        "- 이 실습에서는 nn.Sequential()을 사용하여 간단한 신경망 모델을 구성하는 방법을 학습합니다. 이 모델은 입력 이미지의 크기가 28x28인 경우, Flatten을 통해 1차원 벡터로 변환한 뒤, 두 개의 선형 레이어와 ReLU 활성화 함수를 사용합니다. 마지막으로 10개의 클래스를 출력하는 레이어를 추가합니다. summary() 함수를 사용하여 모델의 구조를 요약하고, 각 레이어의 파라미터 수를 확인합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JV1z01dnBSkZ",
        "outputId": "8fc47190-584e-4267-8a1f-d402ab4dad97"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "==========================================================================================\n",
              "Layer (type:depth-idx)                   Output Shape              Param #\n",
              "==========================================================================================\n",
              "Sequential                               [32, 10]                  --\n",
              "├─Flatten: 1-1                           [32, 784]                 --\n",
              "├─Linear: 1-2                            [32, 128]                 100,480\n",
              "├─ReLU: 1-3                              [32, 128]                 --\n",
              "├─Linear: 1-4                            [32, 10]                  1,290\n",
              "==========================================================================================\n",
              "Total params: 101,770\n",
              "Trainable params: 101,770\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (M): 3.26\n",
              "==========================================================================================\n",
              "Input size (MB): 0.10\n",
              "Forward/backward pass size (MB): 0.04\n",
              "Params size (MB): 0.41\n",
              "Estimated Total Size (MB): 0.54\n",
              "=========================================================================================="
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = nn.Sequential(\n",
        "    nn.Flatten(),                # 28x28 이미지를 1차원 벡터로 펼침\n",
        "    nn.Linear(28*28, 128),       # 입력: 28*28 픽셀, 출력: 128개의 노드\n",
        "    nn.ReLU(),                   # ReLU 활성화 함수\n",
        "    nn.Linear(128, 10)           # 출력층: 10개의 클래스\n",
        ")\n",
        "\n",
        "# 모델 인스턴스 생성\n",
        "model.to(device)\n",
        "summary(model, (32, 1, 28, 28))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qEdkFVk7BSkZ"
      },
      "source": [
        "- 신경망 모델을 구성하고, 요약 정보를 통해 모델의 구조와 파라미터 수를 확인하는 과정을 실습합니다. 이 과정은 모델의 설계를 이해하고 조정하는 데 중요한 단계입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jg8bXwx2BSkZ"
      },
      "source": [
        "## 04-007 nn.Module 기반 신경망 모델 구성\n",
        "- 이 실습에서는 nn.Module을 상속하여 FashionMNIST를 위한 신경망 모델 클래스를 구현합니다. __init__() 메서드에서 Flatten 레이어와 두 개의 선형 레이어를 정의하며, forward() 메서드에서 데이터 흐름을 정의합니다. 마지막 출력 레이어는 CrossEntropyLoss에서 직접 활성화를 처리하므로 활성화 함수를 사용하지 않습니다. summary() 함수를 사용하여 모델의 구조를 요약합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6N8IRJqyBSkZ",
        "outputId": "71ac194c-ec57-41b9-9ac0-c1a45293baf7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "==========================================================================================\n",
              "Layer (type:depth-idx)                   Output Shape              Param #\n",
              "==========================================================================================\n",
              "FashionMNISTModel                        [10, 10]                  --\n",
              "├─Flatten: 1-1                           [10, 784]                 --\n",
              "├─Linear: 1-2                            [10, 128]                 100,480\n",
              "├─Linear: 1-3                            [10, 10]                  1,290\n",
              "==========================================================================================\n",
              "Total params: 101,770\n",
              "Trainable params: 101,770\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (M): 1.02\n",
              "==========================================================================================\n",
              "Input size (MB): 0.03\n",
              "Forward/backward pass size (MB): 0.01\n",
              "Params size (MB): 0.41\n",
              "Estimated Total Size (MB): 0.45\n",
              "=========================================================================================="
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "class FashionMNISTModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FashionMNISTModel, self).__init__()\n",
        "        self.flatten = nn.Flatten()  # 28x28 이미지를 1차원 벡터로 펼침\n",
        "        self.fc1 = nn.Linear(28*28, 128)  # 입력: 28*28 픽셀, 출력: 128개의 노드\n",
        "        self.fc2 = nn.Linear(128, 10)     # 출력층: 10개의 클래스\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        x = torch.relu(self.fc1(x))  # ReLU 활성화 함수 적용\n",
        "        x = self.fc2(x)  # 마지막 출력에는 활성화 함수를 사용하지 않음 (CrossEntropyLoss에서 처리)\n",
        "        return x\n",
        "\n",
        "# 모델 인스턴스 생성\n",
        "model = FashionMNISTModel().to(device)\n",
        "summary(model, (10, 1, 28, 28))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AkfWH_LjBSka"
      },
      "source": [
        "- FashionMNIST를 위한 신경망 모델 클래스를 구현하고, 요약 정보를 통해 모델의 구조와 파라미터 수를 확인하는 과정을 실습합니다. 이 과정은 모델 설계를 이해하고 조정하는 데 중요합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HxyEnqbJBSka"
      },
      "source": [
        "## 04-008 모델 학습 함수 구현\n",
        "- 이 실습에서는 PyTorch 모델을 학습하기 위한 train() 함수를 구현합니다. 모델을 학습 모드로 설정하고, 데이터 로더에서 배치 단위로 이미지를 가져와 모델의 출력과 손실을 계산합니다. 손실에 대해 역전파를 수행하고 옵티마이저를 사용하여 파라미터를 업데이트합니다. 마지막으로, 전체 손실과 정확도를 계산하여 출력합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S8nwpuOKBSka"
      },
      "outputs": [],
      "source": [
        "def train(model, train_loader, criterion, optimizer, epoch, device):\n",
        "    model.train()  # 모델을 학습 모드로 설정\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # 역전파 및 옵티마이저 스텝\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        # 정확도 계산\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    train_loss = running_loss / len(train_loader)\n",
        "    train_accuracy = 100 * correct / total\n",
        "    print(f'Epoch [{epoch+1}]')\n",
        "    print(f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%')\n",
        "\n",
        "    return train_loss, train_accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z0LEfm40BSka"
      },
      "source": [
        "- 모델 학습을 위한 train() 함수를 구현하고, 각 에포크에서 손실과 정확도를 출력하는 과정을 실습합니다. 이 함수는 신경망을 학습시키는 데 필수적인 구성 요소입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FYse6pX9BSka"
      },
      "source": [
        "## 04-009 모델 평가 함수 구현\n",
        "- 이 실습에서는 모델을 평가하기 위한 evaluate() 함수를 구현합니다. 모델을 평가 모드로 설정하고, 기울기 계산을 하지 않도록 torch.no_grad() 블록을 사용합니다. 테스트 데이터 로더에서 배치 단위로 데이터를 가져와 모델의 출력과 손실을 계산합니다. 마지막으로, 전체 테스트 손실과 정확도를 계산하여 출력합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ymP_sU55BSka"
      },
      "outputs": [],
      "source": [
        "# 평가 함수 정의\n",
        "def evaluate(model, test_loader, criterion, device):\n",
        "    model.eval()  # 모델을 평가 모드로 설정\n",
        "    test_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():  # 평가 중에는 기울기 계산을 하지 않음\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            test_loss += loss.item()\n",
        "\n",
        "            # 예측 정확도 계산\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader)\n",
        "    test_accuracy = 100 * correct / total\n",
        "    print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%')\n",
        "\n",
        "    return test_loss, test_accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7nqDkCt5BSkb"
      },
      "source": [
        "- 모델 평가를 위한 evaluate() 함수를 구현하고, 테스트 데이터에서 손실과 정확도를 출력하는 과정을 실습합니다. 이 함수는 모델의 성능을 측정하는 데 필수적입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vH7dcsTRBSkb"
      },
      "source": [
        "## 04-010 학습 및 평가 과정 관리 함수 구현\n",
        "- 이 실습에서는 모델의 학습과 평가 과정을 관리하는 train_and_evaluate() 함수를 구현합니다. 주어진 에포크 수만큼 학습을 반복하고, 매 에포크마다 학습 손실 및 정확도를 기록합니다. 학습 후에는 테스트 데이터셋에서 모델을 평가하고 손실과 정확도를 기록합니다. 최종적으로 학습 및 평가 결과를 리스트로 반환합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yCPxtwyjBSkb"
      },
      "outputs": [],
      "source": [
        "# 학습 및 평가 과정 관리\n",
        "def train_and_evaluate(model, train_loader, test_loader, criterion, optimizer, num_epochs, device):\n",
        "    train_losses = []\n",
        "    train_accuracies = []\n",
        "    test_losses = []\n",
        "    test_accuracies = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # 모델 학습(학습데이터)\n",
        "        train_loss, train_accuracy = train(model, train_loader, criterion, optimizer, epoch, device)\n",
        "        train_losses.append(train_loss)\n",
        "        train_accuracies.append(train_accuracy)\n",
        "\n",
        "        # 모델 평가 (평가데이터)\n",
        "        test_loss, test_accuracy = evaluate(model, test_loader, criterion, device)\n",
        "        test_losses.append(test_loss)\n",
        "        test_accuracies.append(test_accuracy)\n",
        "\n",
        "    return train_losses, train_accuracies, test_losses, test_accuracies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76iTC41wBSkb"
      },
      "source": [
        "- 모델의 학습 및 평가 과정을 관리하는 train_and_evaluate() 함수를 구현하는 과정을 실습합니다. 이 함수는 모델 훈련 및 성능 분석에 중요한 역할을 합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8OMjmrz8BSkc"
      },
      "source": [
        "## 04-011 손실 함수 및 옵티마이저 설정과 학습 수행\n",
        "- 이 실습에서는 모델 학습을 위한 손실 함수와 옵티마이저를 설정합니다. nn.CrossEntropyLoss()는 다중 클래스 분류 문제에 적합한 손실 함수로, 각 클래스의 확률을 기반으로 손실을 계산합니다. Adam 옵티마이저는 학습률(lr)을 0.01로 설정하여 모델의 파라미터를 업데이트합니다. 이후, train_and_evaluate() 함수를 호출하여 모델을 학습하고 평가합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jjIbLivuBSkc"
      },
      "outputs": [],
      "source": [
        "# 손실 함수와 옵티마이저 설정\n",
        "criterion = nn.CrossEntropyLoss()  # 다중 클래스 분류를 위한 손실 함수\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)  # Adam 옵티마이저\n",
        "\n",
        "train_losses, train_accuracies, test_losses, test_accuracies = train_and_evaluate(\n",
        "    model, train_loader, test_loader, criterion, optimizer, num_epochs=10, device=device\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hSjs4kdyBSkc"
      },
      "source": [
        "- 손실 함수와 옵티마이저를 설정한 후, 모델을 학습하고 평가하는 과정을 실습합니다. 이 과정은 신경망 모델의 성능을 최적화하는 데 필수적입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NL2o9VP_BSkc"
      },
      "source": [
        "## 04-012 드롭아웃을 포함한 FashionMNIST 모델 클래스 구현\n",
        "- 이 실습에서는 드롭아웃(Dropout) 기법을 포함한 FashionMNIST 모델 클래스를 구현합니다. nn.Dropout()을 사용하여 과적합을 방지하기 위해 첫 번째 은닉층 뒤에 드롭아웃을 적용합니다. 드롭아웃 비율은 0.1로 설정하여 학습 중 무작위로 뉴런을 비활성화합니다. 모델의 구조는 summary() 함수를 통해 확인합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_b9bH9D2BSkc"
      },
      "outputs": [],
      "source": [
        "# 신경망 모델 정의 (Dropout 포함)\n",
        "class FashionMNISTDropoutModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FashionMNISTDropoutModel, self).__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc1 = nn.Linear(28*28, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "        self.dropout = nn.Dropout(0.1)  # 10%의 드롭아웃 적용\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.dropout(x)  # 첫 번째 은닉층 뒤에 드롭아웃 적용\n",
        "        x = self.fc2(x)  # 출력층에는 드롭아웃을 사용하지 않음\n",
        "        return x\n",
        "\n",
        "# 모델 인스턴스화\n",
        "model = FashionMNISTDropoutModel().to(device)\n",
        "summary(model, (10, 1, 28, 28))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1PBPk7GBSkc"
      },
      "source": [
        "- 드롭아웃을 포함한 FashionMNIST 신경망 모델을 구현하고, 요약 정보를 통해 모델의 구조와 파라미터 수를 확인하는 과정을 실습합니다. 드롭아웃은 신경망의 일반화 성능을 향상시키는 데 효과적인 기법입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZHr2c2n8BSkd"
      },
      "source": [
        "## 04-013 손실 함수 및 옵티마이저 설정과 학습 수행\n",
        "- 이 실습에서는 모델 학습을 위한 손실 함수와 옵티마이저를 설정합니다. nn.CrossEntropyLoss()는 다중 클래스 분류 문제에 적합한 손실 함수로, 각 클래스의 확률을 기반으로 손실을 계산합니다. Adam 옵티마이저는 학습률(lr)을 0.01로 설정하여 모델의 파라미터를 업데이트합니다. 이후, train_and_evaluate() 함수를 호출하여 모델을 학습하고 평가합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DgkepgvkBSkd"
      },
      "outputs": [],
      "source": [
        "# 손실 함수와 옵티마이저 설정\n",
        "criterion = nn.CrossEntropyLoss()  # 다중 클래스 분류를 위한 손실 함수\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)  # Adam 옵티마이저\n",
        "\n",
        "train_losses, train_accuracies, test_losses, test_accuracies = train_and_evaluate(\n",
        "    model, train_loader, test_loader, criterion, optimizer, num_epochs=10, device=device\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YrF0cpkcBSkd"
      },
      "source": [
        "- 손실 함수와 옵티마이저를 설정한 후, 모델을 학습하고 평가하는 과정을 실습합니다. 이 과정은 신경망 모델의 성능을 최적화하는 데 필수적입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p81iDIRzBSkd"
      },
      "source": [
        "## 04-014 Batch Normalization 및 Dropout 포함 모델 정의\n",
        "- 이 실습에서는 Batch Normalization과 Dropout을 포함한 FashionMNIST 모델을 정의합니다. nn.BatchNorm1d()를 사용하여 첫 번째 선형 레이어의 출력을 정규화하고, nn.Dropout()을 통해 과적합을 방지하기 위해 드롭아웃을 적용합니다. 모델의 구조는 summary() 함수를 통해 확인합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mrpx_fI3BSkd"
      },
      "outputs": [],
      "source": [
        "# 신경망 모델 정의 (Batch Normalization 및 Dropout 포함)\n",
        "class FashionMNISTBNModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FashionMNISTBNModel, self).__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc1 = nn.Linear(28*28, 128)\n",
        "        self.bn1 = nn.BatchNorm1d(128)  # 첫 번째 배치 정규화 레이어\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "        self.dropout = nn.Dropout(0.1)  # 10%의 드롭아웃 적용\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        x = torch.relu(self.bn1(self.fc1(x)))  # 첫 번째 배치 정규화 + ReLU\n",
        "        x = self.dropout(x)  # 드롭아웃 적용\n",
        "        x = self.fc2(x)  # 출력층에는 배치 정규화 및 드롭아웃을 사용하지 않음\n",
        "        return x\n",
        "\n",
        "# 모델 인스턴스화\n",
        "model = FashionMNISTBNModel().to(device)\n",
        "summary(model, (10, 1, 28, 28))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "419Dnn6dBSkd"
      },
      "source": [
        "- Batch Normalization 및 Dropout을 포함한 FashionMNIST 신경망 모델을 구현하고, 요약 정보를 통해 모델의 구조와 파라미터 수를 확인하는 과정을 실습합니다. 이 기법들은 신경망의 학습 성능을 향상시키는 데 도움을 줍니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xGTnfyIoBSke"
      },
      "source": [
        "## 04-015 손실 함수 및 옵티마이저 설정과 학습 수행\n",
        "- 이 실습에서는 모델 학습을 위한 손실 함수와 옵티마이저를 설정합니다. nn.CrossEntropyLoss()는 다중 클래스 분류 문제에 적합한 손실 함수로, 각 클래스의 확률을 기반으로 손실을 계산합니다. Adam 옵티마이저는 학습률(lr)을 0.01로 설정하여 모델의 파라미터를 업데이트합니다. 이후, train_and_evaluate() 함수를 호출하여 모델을 학습하고 평가합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PwTWmTR0BSke"
      },
      "outputs": [],
      "source": [
        "# 손실 함수와 옵티마이저 설정\n",
        "criterion = nn.CrossEntropyLoss()  # 다중 클래스 분류를 위한 손실 함수\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)  # Adam 옵티마이저\n",
        "\n",
        "train_losses, train_accuracies, test_losses, test_accuracies = train_and_evaluate(\n",
        "    model, train_loader, test_loader, criterion, optimizer, num_epochs=10, device=device\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aq5j8rs3BSke"
      },
      "source": [
        "- 손실 함수와 옵티마이저를 설정한 후, 모델을 학습하고 평가하는 과정을 실습합니다. 이 과정은 신경망 모델의 성능을 최적화하는 데 필수적입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zvritV4jBSke"
      },
      "source": [
        "## 04-016 L2 정칙화를 위한 옵티마이저 설정\n",
        "- 이 실습에서는 L2 정칙화(L2 regularization)를 적용하기 위해 SGD(Stochastic Gradient Descent) 옵티마이저를 설정합니다. weight_decay 매개변수를 사용하여 L2 정칙화 항을 추가함으로써 과적합을 방지할 수 있습니다. 이는 모델의 일반화 성능을 향상시키는 데 중요한 역할을 합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T0hzxKLeBSke"
      },
      "outputs": [],
      "source": [
        "# L2 정칙화를 위한 옵티마이저 설정 (weight_decay가 L2 정칙화)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, weight_decay=0.001)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mK7-7HAIBSkf"
      },
      "source": [
        "- L2 정칙화를 적용한 옵티마이저를 설정하는 과정을 실습합니다. 이는 신경망 학습에서 모델의 일반화 성능을 개선하는 데 효과적입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rK1cu2o2BSkf"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LcqZX9WpBSkf"
      },
      "source": [
        "## 연습문제-04-001 Sequential 방식으로 2차원 더미 데이터를 이용한 다중 분류 모델 설계\n",
        "- 이 실습에서는 PyTorch의 Sequential API를 사용하여 다중 분류 모델을 설계하는 방법을 학습합니다. Sequential 방식은 모델의 각 계층을 순차적으로 정의하며, 간단한 네트워크를 설계할 때 유용합니다.\n",
        "\n",
        "먼저, 2차원 입력을 받는 네트워크를 설계하게 됩니다. 첫 번째 층에서는 입력 데이터(2차원)를 32개의 노드로 확장합니다. 이어서 ReLU 활성화 함수를 적용하여 비선형성을 추가합니다. 두 번째 층에서는 32개의 노드를 16개의 노드로 줄이며, 마찬가지로 ReLU 활성화 함수를 적용합니다. 마지막으로, 출력층에서 1개의 클래스를 분류하는 sigmoid 함수가 적용됩니다.\n",
        "\n",
        "이 실습을 통해 이진 분류 문제에서 Sequential API를 어떻게 활용하는지, 각 층의 역할과 활성화 함수가 모델 성능에 어떻게 기여하는지에 대해 학습할 수 있습니다. 또한, sigmoid 함수가 분류 문제에서 어떻게 확률 기반의 출력을 생성하는지 이해하게 됩니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Wr1ILb5BSkf"
      },
      "outputs": [],
      "source": [
        "# 첫 번째 모델은 nn.Sequential을 사용하여 정의됨.\n",
        "# nn.Sequential은 모델의 레이어를 순차적으로 쌓을 수 있는 간단한 방식.\n",
        "\n",
        "\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(2, 32),  # 입력 2차원, 출력 32차원\n",
        "    nn.ReLU(),         # 활성화 함수\n",
        "    nn.Linear(32, 16), # 출력 16차원\n",
        "    nn.ReLU(),         # 활성화 함수\n",
        "    nn.Linear(16, 1),  # 출력 1차원\n",
        "    nn.sigmoid()       # 시그모이드 함수로 확률 분포 반환\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZ6YzqlrBSkf"
      },
      "source": [
        "- PyTorch Sequential API를 사용하여 2차원 더미 데이터를 기반으로 다중 분류 DNN 모델을 설계하고, 각 계층의 역할과 모델 구조를 학습합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBzV_WdmBSkg"
      },
      "source": [
        "## 연습문제-04-002 Module 방식으로 2차원 더미 데이터를 이용한 회귀 모델 설계\n",
        "- 이 실습에서는 PyTorch의 Module 방식을 사용하여 회귀 문제를 해결하는 모델을 설계하는 방법을 학습합니다. Module 방식을 사용하면, 모델의 계층을 더 세밀하게 정의할 수 있으며, 복잡한 네트워크 구조나 맞춤형 로직을 적용할 때 유리합니다.\n",
        "\n",
        "모델은 2차원 입력을 받으며, 첫 번째 층에서 64개의 노드로 확장한 뒤 ReLU 활성화 함수를 적용합니다. 두 번째 층에서는 64개의 노드를 32개의 노드로 줄이고, 마찬가지로 ReLU 활성화 함수를 사용하여 비선형성을 부여합니다. 마지막으로, 출력층에서 단일 회귀 값을 예측합니다.\n",
        "\n",
        "이 실습을 통해 PyTorch의 Module 클래스를 사용하여 회귀 모델을 설계하고, 각 계층의 역할 및 활성화 함수가 어떻게 회귀 문제 해결에 기여하는지 학습할 수 있습니다. 또한, ReLU 활성화 함수가 네트워크 학습에서 어떤 역할을 하는지, 회귀 문제에서 출력층이 어떻게 동작하는지 이해하게 됩니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wWZW8dn3BSkg"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Module 방식으로 회귀 모델 설계\n",
        "class RegressionModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(RegressionModel, self).__init__()\n",
        "        self.layer1 = nn.Linear(2, 64)\n",
        "        self.layer2 = nn.Linear(64, 32)\n",
        "        self.layer3 = nn.Linear(32, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.layer1(x))\n",
        "        x = torch.relu(self.layer2(x))\n",
        "        x = self.layer3(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pjVVO_fmBSkg"
      },
      "source": [
        "- PyTorch Module 클래스를 사용하여 2차원 더미 데이터를 기반으로 회귀 모델을 설계하고, 각 계층의 역할과 활성화 함수의 중요성을 학습합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W77ZcCZeBSkg"
      },
      "source": [
        "---"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}